{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Measuring Demand Forecasting benefits series**\n",
    "\n",
    "# Generating forecasts with Amazon SageMaker Canvas\n",
    "\n",
    "> *This notebook should work with the **`Data Science 3.0`** kernel in SageMaker Studio (older versions may see errors)*\n",
    ">\n",
    "> ⚠️ ***If** running the optional data preparation section, you'll need to switch to a larger instance size such as `ml.m5.large` (2 vCPU + 8 GiB RAM). Otherwise, any instance type should be fine.*\n",
    "\n",
    "In this notebook we'll use [Amazon SageMaker Canvas](https://aws.amazon.com/sagemaker/canvas/) to build and export a forecast through the no-code, business analyst-oriented Canvas UI.\n",
    "\n",
    "> ⚠️ **A note on cost:** At the time of writing, this example's prepared dataset is approximately 27M cells which, per [the pricing page](https://aws.amazon.com/sagemaker/canvas/pricing/), may cost upwards of $500 to train a model in Canvas. If you're just exploring at this stage and have no strong need for a business-user-friendly UI, you could consider the [Amazon Forecast notebook](1.3.%20Run%20Amazon%20Forecast.ipynb) instead for a lower-cost option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contents\n",
    "\n",
    "1. [Canvas pre-requisites](#prereqs)\n",
    "1. [(Optional) Data preparation](#dataprep)\n",
    "    - [Dependencies and setup](#Dependencies-and-setup)\n",
    "    - [Target Time-Series (TTS)](#tts)\n",
    "    - [Related Time-Series (RTS)](#rts)\n",
    "    - [The combined SageMaker Canvas dataset](#The-combined-SageMaker-Canvas-dataset)\n",
    "1. [Import your dataset to SageMaker Canvas](#Import-your-dataset-to-SageMaker-Canvas)\n",
    "1. [Configure and train your model](#Configure-and-train-your-model)\n",
    "1. [Review and export forecast results](#Review-and-export-forecast-results)\n",
    "1. [Next steps](#Next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canvas pre-requisites<a class=\"anchor\" id=\"prereqs\"></a>\n",
    "\n",
    "To follow the instructions below, you'll need SageMaker Canvas set up: See the [\"getting started\"](https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-getting-started.html#canvas-prerequisites) and [\"setting up\"](https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-setting-up.html) sections of the developer guide for steps.\n",
    "\n",
    "In particular, you'll also need to [enable time-series forecasting](https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-set-up-forecast.html) for your Canvas user(s).\n",
    "\n",
    "If skipping the optional data preparation section below, you'll also need to [enable upload of local files](https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-set-up-local-upload.html) through the Canvas UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## (Optional) Data preparation<a class=\"anchor\" id=\"dataprep\"></a>\n",
    "\n",
    "> ℹ️ **You don't need to run any of the code in this section.**\n",
    ">\n",
    "> We've already prepared a Canvas-ready file for the sample dataset and made it available for [download here](https://measuring-forecast-benefits-assets.s3.amazonaws.com/dataset/v1/benefits_demo_canvas.csv). This section is provided to illustrate how the file was generated, in case you'd like to re-use elements of the process for your own custom datasets.\n",
    "\n",
    "To build a forecast with SageMaker Canvas, we need to prepare our dataset in line with the [format requirements documented in the developer guide](https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-time-series.html).\n",
    "\n",
    "To be specific, we'll want a CSV in \"flat file\" format (date as one column, not in 2D \"pivot table\" style) where our target value finishes at the end of the historical period, but any additional input variables continue on to cover the forecast period too - similar to the format shown below:\n",
    "\n",
    "| **timestamp** | **item_id** | **location** | **demand** | **weekend_hol_flag** | **promo** | **unit_price** |\n",
    "|:-------------:|:-----------:|:------------:|-----------:|---------------------:|----------:|---------------:|\n",
    "| 2017-01-01 | Hoodie_gray_L | Brazil_Store1 | 12 | 2 | 1.0 | 24.20 |\n",
    "| 2017-01-02 | Hoodie_gray_L | Brazil_Store1 | 0 | 0 | 1.0 | 24.20 |\n",
    "| ... | ... | ... | ... | ... | ... | ... |\n",
    "| 2019-11-30 | Hoodie_gray_L | Brazil_Store1 | 4 | 0 | 1.0 | 24.20 |\n",
    "| **2019-12-01** | Hoodie_gray_L | Brazil_Store1 |  | 0 | 1.0 | 24.20 |\n",
    "| **...** | ... | ... | ... | ... | ... | ... |\n",
    "| **2019-12-30** | Hoodie_gray_L | Brazil_Store1 |  | 0 | 1.0 | 24.20 |\n",
    "| **2019-12-31** | Hoodie_gray_L | Brazil_Store1 |  | 0 | 1.0 | 24.20 |\n",
    "\n",
    "Note that in our sample dataset, historical actual sales finish on 2019-12-31. Other notebooks in this series forecast for January 2020 but simultaneously collect evaluation data for December 2019, but SageMaker Canvas doesn't (at time of writing) have an equivalent for Amazon Forecast's \"backtest export\" feature.\n",
    "\n",
    "In this notebook we'll use Python to prepare this file from the source data for consistency with the rest of the series - but you could use other no-code data tools if you prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and setup\n",
    "\n",
    "First we'll import the libraries this notebook needs, and configure the forecast horizon and Amazon S3 data location.\n",
    "\n",
    "To be able to compare our \"forecast\" with actuals and the (monthly) baseline rolling average model, we'll actually be predicting the final month covered by the dataset: December 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# External Dependencies:\n",
    "import pandas as pd  # Tabular/dataframe processing tools\n",
    "import sagemaker  # SageMaker SDK used just to look up default S3 bucket\n",
    "\n",
    "# Local Dependencies:\n",
    "import util\n",
    "\n",
    "# Configuration:\n",
    "BUCKET_NAME = sagemaker.Session().default_bucket()\n",
    "BUCKET_PREFIX = \"measuring-forecast-benefits/\"\n",
    "\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "HISTORY_START = datetime(year=2017, month=1, day=1)\n",
    "FORECAST_START = datetime(year=2019, month=12, day=1)\n",
    "\n",
    "FORECAST_HORIZON = pd.offsets.Day() * 31  # or e.g. Hour(), Week(), MonthEnd()\n",
    "print(f\"Configured forecast horizon: {FORECAST_HORIZON}\")\n",
    "FORECAST_FREQ = FORECAST_HORIZON.base.freqstr  # This should be Amazon Forecast compatible\n",
    "print(f\"({FORECAST_HORIZON.n} units of '{FORECAST_FREQ}')\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Target Time-Series (TTS)<a class=\"anchor\" id=\"tts\"></a>\n",
    "\n",
    "The thing we want to predict is called the \"target\": In this case, the number of units we can sell of each item or the \"demand\". Historical sales data will provide this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tts_df = util.analytics.filter_to_period(\n",
    "    pd.read_parquet(\"s3://measuring-forecast-benefits-assets/dataset/v1/sales.parquet\"),\n",
    "    period_start=HISTORY_START,\n",
    "    period_end=FORECAST_START,  # (Exclusive)\n",
    "    timestamp_col_name='date',\n",
    ")\n",
    "\n",
    "tts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is already in line with our [target format](https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-time-series.html), so we'll simply *index* it by the key columns (timestamp, SKU, location) to enable joining with other variables later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FORECAST_DIMENSIONS = [c for c in tts_df.columns if c not in (\"date\", \"sales\", \"sku\")]\n",
    "print(f\"Forecast Dimensions:\\n  {FORECAST_DIMENSIONS}\")\n",
    "\n",
    "tts_df = tts_df.set_index([\"date\", \"sku\"] + FORECAST_DIMENSIONS)\n",
    "tts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related Time-Series (RTS)<a class=\"anchor\" id=\"rts\"></a>\n",
    "\n",
    "We can include other input signals that **vary over time** to help the model predict the target, which for consistency with Amazon Forecast we'll call \"related time-series\". Popular time-varying features to help predict future demand include pricing and promotions, public holidays and events, and even weather information.\n",
    "\n",
    "In this sample we'll build related time-series from two base datasets: Public holidays by country, and product prices/promotions. To use them with SageMaker Canvas, we'll need to consolidate them together into the same data file together with the target (TTS) from earlier.\n",
    "\n",
    "To understand *which* reference data we need, we'll first extract the unique list of all item+location combinations in the scope of the dataset, and map those to countries (for public holidays) and product types (for price data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_location_combos = tts_df.reset_index()[[\"sku\", \"location\"]].drop_duplicates()\n",
    "item_location_combos[\"product\"] = item_location_combos[\"sku\"].str.split(\"_\").str[0]\n",
    "item_location_combos[\"country\"] = item_location_combos[\"location\"].str.split(\"_\").str[0]\n",
    "item_location_combos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Weekends and holidays\n",
    "\n",
    "The source weekend and holiday data has already been prepared in a flat file format. However, it extends for a full year beyond our TTS end date - so we need to trim it for only the forecasting period of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "holiday_raw_df = pd.read_csv(\n",
    "    \"s3://measuring-forecast-benefits-assets/dataset/v1/weekend_holiday_flag.csv\",\n",
    ")\n",
    "holiday_raw_df[\"date\"] = pd.to_datetime(holiday_raw_df[\"date\"])  # (As CSV)\n",
    "\n",
    "# Filter out any data beyond the end of the forecasting horizon:\n",
    "holiday_raw_df = util.analytics.filter_to_period(\n",
    "    holiday_raw_df,\n",
    "    period_start=HISTORY_START,\n",
    "    period_end=FORECAST_START + FORECAST_HORIZON,\n",
    "    timestamp_col_name=\"date\",\n",
    ")\n",
    "\n",
    "holiday_raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prices and promotions\n",
    "\n",
    "The price and promotion data is likewise available in a flat file format already, but needs some extra processing:\n",
    "\n",
    "Typically, teams will have historical price data from one source and need to add in additional data to *project forward* what expected pricing will be over the forecast period - perhaps even modelling multiple scenarios with different forecast models.\n",
    "\n",
    "That's not actually the case here (because we're building a retrospective \"forecast\" for a period where we already have historical data), but we'll simulate it by filtering out the final month of historical data:\n",
    "\n",
    "> ⚠️ **Note:** This is not quite aligned with the [Amazon Forecast notebook](1.3.%20Run%20Amazon%20Forecast.ipynb), where we forecast 2020-01 (with projected prices) and back-test 2019-12 (with actual prices) - which should slightly disadvantage SageMaker Canvas in the final comparison. You could instead adjust the `period_end` filter here and skip the future projection below if you want a more direct comparison. Since SageMaker Canvas forecasting *uses Amazon Forecast under the hood anyway* though, we felt it was more important to show a realistic workflow than focus on comparing between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the real/historical pricing:\n",
    "prices_raw_df = util.analytics.filter_to_period(\n",
    "    pd.read_parquet(\"s3://measuring-forecast-benefits-assets/dataset/v1/prices_promos.parquet\"),\n",
    "    period_start=HISTORY_START,\n",
    "    period_end=FORECAST_START,  # (Omitting `+ FORECAST_HORIZON` as noted above)\n",
    "    timestamp_col_name=\"date\",\n",
    ")\n",
    "prices_raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To project final prices forward, we'll first create a dataframe of empty (`NaN`) placeholders for all the future dates and products/countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prices_dimensions = [\n",
    "    c for c in prices_raw_df.columns if c not in (\"date\", \"promo\", \"unit_price\")\n",
    "]\n",
    "\n",
    "prices_future = pd.merge(\n",
    "    # Range of dates in the forecast period:\n",
    "    pd.date_range(\n",
    "        FORECAST_START,\n",
    "        FORECAST_START + FORECAST_HORIZON,\n",
    "        freq=FORECAST_FREQ,\n",
    "        inclusive=\"left\",\n",
    "        name=\"date\",\n",
    "    ).to_series(),\n",
    "    # Unique combinations of country+product:\n",
    "    prices_raw_df[prices_dimensions].drop_duplicates(),\n",
    "    # Cross join (all combinations):\n",
    "    how=\"cross\",\n",
    ")\n",
    "prices_future[\"promo\"] = float(\"nan\")\n",
    "prices_future[\"unit_price\"] = float(\"nan\")\n",
    "prices_future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can combine the past dataset and future placeholders to fill forward each product's final historical price across the full forecast window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prices_projected_df = pd.concat(\n",
    "    # Concatenate the future placeholders with the historical prices\n",
    "    [prices_raw_df, prices_future]\n",
    ").set_index(\n",
    "    # Index by key dimensions *before* date...\n",
    "    prices_dimensions + [\"date\"]\n",
    ").sort_index().groupby(\n",
    "    # ...so we can sort by product/SKU first, and then ffill() any gaps\n",
    "    level=prices_dimensions\n",
    ").ffill().reset_index()\n",
    "\n",
    "prices_projected_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like, you can inspect this DataFrame to validate the continuity (i.e. `Brazil` `Gloves` will keep using the same `promo` and `unit_price` for records after `TTS_PERIOD_END` - and likewise for each other combination of country and product type).\n",
    "\n",
    "> ℹ️ If you instead loaded actual prices/promotions for the full forecast period by setting `period_end=FORECAST_START + FORECAST_HORIZON` above, you can just set `prices_projected_df = prices_raw_df` before moving on.\n",
    "\n",
    "We can now delete temp variables no longer needed to save memory and make sure we don't accidentally use the wrong ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del prices_future\n",
    "del prices_raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pulling the RTS together\n",
    "\n",
    "With our end dates aligned and set up to fully cover the forecast period, we're ready to combine the two datasets and normalize the dimensions to match the Target Time-Series.\n",
    "\n",
    "First, we'll join them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rts_df = pd.merge(\n",
    "    holiday_raw_df,\n",
    "    prices_projected_df,\n",
    "    on=[\"date\", \"country\"],\n",
    "    how=\"outer\",\n",
    ")\n",
    "\n",
    "del holiday_raw_df\n",
    "del prices_projected_df\n",
    "\n",
    "rts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then expand from `country` to cover all separate `location` IDs and from `product` to cover all separate SKUs. We can refer to the unique location/item_id list saved from the TTS earlier to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join to map country/product to locations/item_ids:\n",
    "rts_df = (\n",
    "    pd.merge(\n",
    "        item_location_combos,\n",
    "        rts_df,\n",
    "        on=[\"country\", \"product\"],\n",
    "        how=\"outer\",\n",
    "    )\n",
    "    .drop(columns=[\"country\", \"product\"])\n",
    ")\n",
    "\n",
    "del item_location_combos\n",
    "\n",
    "# Standardize timestamp representation, as with TTS:\n",
    "#rts_df[\"timestamp\"] = rts_df[\"timestamp\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Index on key fields:\n",
    "rts_df = rts_df.set_index([\"date\", \"sku\"] + FORECAST_DIMENSIONS)\n",
    "\n",
    "rts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The combined SageMaker Canvas dataset\n",
    "\n",
    "Now we have our target (sales) data covering the history up to 2019-11-30, and our related input data covering the history *and forecast horizon* up to 2019-12-31.\n",
    "\n",
    "While Amazon Forecast treats these as [two separate datasets](https://docs.aws.amazon.com/forecast/latest/dg/howitworks-datasets-groups.html), for Amazon SageMaker Canvas we'll want to combine them together into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "canvas_df = tts_df.join(rts_df, how=\"outer\")\n",
    "\n",
    "del tts_df\n",
    "del rts_df\n",
    "\n",
    "canvas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice some **missing values** in the `demand` column after this join:\n",
    "\n",
    "1. Missing values **on or after** the `FORECAST_START` date are fine and expected: These will tell Canvas that it needs to forecast for this period but has the other features (holidays and prices) available to help.\n",
    "2. Missing values **before** the `FORECAST_START` date should be filled with zeros: These emerge because the source sales data is sparse - there simply won't be a record if no units of a product were sold for a particular date/SKU/location/etc combination.\n",
    "\n",
    "There should not be any missing values in the other columns. We can run some diagnostics here to check everything is as expected before filling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Last date with historical sales:\",\n",
    "    canvas_df.index.get_level_values(\"date\")[~canvas_df[\"sales\"].isna()].max(),\n",
    "    \"(should be one day before:\",\n",
    "    FORECAST_START,\n",
    "    \")\"\n",
    ")\n",
    "\n",
    "print(\"Missing value counts by column:\")\n",
    "canvas_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then fill all empty sales figures **before** the `FORECAST_START` with 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "canvas_df.loc[\n",
    "    (\n",
    "        slice(None, FORECAST_START - FORECAST_HORIZON.base),  # (End is inclusive so minus one day)\n",
    "        canvas_df[\"sales\"].isna(),\n",
    "    ),\n",
    "    \"sales\",\n",
    "] = 0\n",
    "canvas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is now ready to use with SageMaker Canvas forecasting. We'll upload it to Amazon S3 directly to use in the tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "canvas_s3_uri = f\"s3://{BUCKET_NAME}/{BUCKET_PREFIX}training-data/canvas/benefits_demo_canvas.csv\"\n",
    "canvas_df.to_csv(canvas_s3_uri)\n",
    "print(f\"Uploaded SageMaker Canvas dataset to: {canvas_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import your dataset to SageMaker Canvas\n",
    "\n",
    "1. **If** you ran through the optional data preparation steps above, take note of the final `s3://...` URI of your uploaded dataset.\n",
    "1. **Otherwise, you can either**:\n",
    "    1. **Run the code cell below** to transfer the public dataset into your AWS account's Amazon S3 bucket, or\n",
    "    1. **Download** the (>200MiB) [pre-prepared data from this link](https://measuring-forecast-benefits-assets.s3.amazonaws.com/dataset/v1/benefits_demo_canvas.csv) to your computer, to upload into Canvas later\n",
    "\n",
    "Since SageMaker notebooks run directly in the AWS Region you deploy them, transferring this large dataset via the notebook may be faster than downloading to your computer. As mentioned up in the [Pre-requisites section](#prereqs) at the beginning of this notebook, you'll need to enable additional permissions in SageMaker Canvas for users to upload local files, if you haven't already.\n",
    "\n",
    "Either way, since the transfer may take a few minutes, you can go ahead and launch SageMaker Canvas (steps below) while it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "canvas_s3_uri = \"s3://{}/{}training-data/canvas/benefits_demo_canvas.csv\".format(\n",
    "    sagemaker.Session().default_bucket(),  # S3 bucket name as per BUCKET_NAME above\n",
    "    \"measuring-forecast-benefits/\",  # S3 folder prefix as per BUCKET_PREFIX above\n",
    ")\n",
    "\n",
    "!aws s3 cp s3://measuring-forecast-benefits-assets/dataset/v1/benefits_demo_canvas.csv {canvas_s3_uri}\n",
    "\n",
    "print(f\"Uploaded SageMaker Canvas dataset to: {canvas_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way you Canvas will vary depending on how your SageMaker Domain authentication is set up: See the [getting started guide](https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-getting-started.html) for details. For an IAM-authenticated domain as created through \"Quick setup\", you'll typically launch Canvas from the [Amazon SageMaker Console](https://console.aws.amazon.com/sagemaker/home?#/canvas-landing) page for Canvas or for your Domain:\n",
    "\n",
    "![](img/canvas-01-launch.png \"Screenshot of SageMaker Domain users page with option to launch SageMaker Canvas\")\n",
    "\n",
    "> ℹ️ **Tip:** Launching the Canvas UI for a user starts a \"session\", which may take a minute or two and (as detailed on the [SageMaker Canvas pricing page](https://aws.amazon.com/sagemaker/canvas/pricing/)) is chargeable for the time it's running. Simply closing your browser tab will not shut down the session: Remember to click \"log out\" in the Canvas UI when you're done, and consider setting up [an automatic shutdown solution](https://aws.amazon.com/blogs/machine-learning/save-costs-by-automatically-shutting-down-idle-resources-within-amazon-sagemaker-studio/) if you manage a domain with several users. You can check the running \"apps\" for your Studio user at any time by clicking the blue user names in the screen above, to see whether a Canvas session is open."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Canvas UI, navigate to **Datasets** in the sidebar menu. You'll usually see a set of sample datasets pre-loaded in a new domain.\n",
    "\n",
    "Click the **➕ Import** button as shown below to import your own dataset.\n",
    "\n",
    "![](img/canvas-02-datasets.png \"Screenshot of SageMaker Canvas UI Datasets page showing Import button\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data is already uploaded to your Amazon S3 account from the code steps above, browse to and select it as shown below. If you instead downloaded the file to your computer, select \"Upload\" and then drag+drop the file from your computer to upload it.\n",
    "\n",
    "Once your file is uploaded or selected, click **Import data** to continue.\n",
    "\n",
    "![](img/canvas-03-import.png \"Screenshot of Canvas UI showing target data selected in Amazon S3, with alternative Upload tab also visible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you import, Canvas will save your data and analyze the shape - the processing for which might take a few seconds.\n",
    "\n",
    "> ⚠️ **Understanding costs:** As well as active sessions, [SageMaker Canvas Pricing](https://aws.amazon.com/sagemaker/canvas/pricing/) charges for model training based on the number of cells in your dataset. The metrics on this dataset list are useful for understanding the cost before training models on your data.\n",
    ">\n",
    "> Note that the dataset used in this example could incur significant charges for model training: Around $570 by our estimate at the time of pricing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and train your model\n",
    "\n",
    "When the dataset shows as \"Ready\", you'll be able to select it and click **Create a model** to start building your model.\n",
    "\n",
    "![](img/canvas-04-select-data.png \"Screenshot selecting data from datasets list in Canvas, showing button to Create a model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll be asked to **enter a name** for your model, and can select one as you like. In our example we used `benefits_demo_canvas`.\n",
    "\n",
    "On the following screen, you'll see a preview of your data and need to configure your model:\n",
    "\n",
    "- Set **Target column** to `sales`\n",
    "- Your **Model type** should be automatically detected as **Time series forecasting**. If not, you can use the \"Change type\" button to override this.\n",
    "\n",
    "Note that from this screen you can toggle between (list of) columns view and \"grid\" view in the dataset preview, and also access other basic data analysis and preparation options. We won't need them here, as our dataset is already prepared:\n",
    "\n",
    "![](img/canvas-05-model-type.png \"Screenshot of Canvas model build with target column selected and time-series model type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click **Configure time series model** and, as shown in the screenshot below:\n",
    "\n",
    "- For **Item ID column** select `sku`\n",
    "- For **Group column** select `location`\n",
    "- For **Time stamp column** select `date`\n",
    "- For **Number of days to forecast** enter `31`\n",
    "- Leave other options as default\n",
    "\n",
    "![](img/canvas-06-configure-model.png \"Screenshot configuring the forecasting model key fields and horizon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these options saved, you may be prompted to **Validate your data**: Accept this for Canvas to check your dataset matches data quality expectations.\n",
    "\n",
    "Once your data is validated, you should be able to click **Standard build** to start your model training. Note that for some datasets you'll have an alternative \"Quick build\" option, but at the time of writing this dataset doesn't support that feature:\n",
    "\n",
    "![](img/canvas-07-start-build.png \"Screenshot showing enabled Standard Build button to start model training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⏰ In our experience, a standard build model for this sample dataset will normally take 4-5 hours to train once you start the process.\n",
    "\n",
    "This is similar to the training time for Amazon Forecast, and in fact if you head over to the [Amazon Forecast console](https://console.aws.amazon.com/forecast/home?#datasetGroups) you'll see that Canvas even uses Amazon Forecast under the hood!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Review and export forecast results\n",
    "\n",
    "Once the model is ready, you'll see accuracy metrics and column impact scores on the *Analyze* tab - which can be useful for understanding performance of the model at a high level and which additional columns were most useful for prediction:\n",
    "\n",
    "![](img/canvas-08-metrics.png \"Screenshot of model metrics and feature importance scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over in the **Predict** tab, you can optionally explore predictions for **single** SKU/location combinations in the dataset as shown below. Note that the model gives probabilistic forecasts, with upper-bound and lower-bound confidence estimates as well as the expected value:\n",
    "\n",
    "![](img/canvas-09-predict-single.png \"Screenshot of single-item prediction in SageMaker Canvas showing higher-bound, lower-bound and median quantiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze and compare results in the next notebook though, you'll need to select **All items** and then click **Start Predictions** to start building a combined forecast for all products and locations.\n",
    "\n",
    "> ⏰ Once you start the job, it will take Canvas a few minutes to generate the full set of predictions.\n",
    "\n",
    "▶️ **Download** your batch predictions from the Canvas UI (about 10MiB in our test), once they're ready, as shown below:\n",
    "\n",
    "![](img/canvas-10-predict-batch.png \"Screenshot of Canvas batch predictions with job complete and results ready to download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Next steps\n",
    "\n",
    "You should now have successfully trained a SageMaker Canvas forecasting model to predict sales in the final month of the sample dataset (2019-12) - and downloaded the results to a local CSV file.\n",
    "\n",
    "Next, head on over to notebook [2. Measuring Forecast Benefits.ipynb](2.%20Measuring%20Forecast%20Benefits.ipynb) - where we'll compare this forecast to the moving average baseline in terms of the actual business results of applying them in practice."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ddbda436de216a15b983650f9bacf1dadb709a40e27f1fee3bde2bf4145658fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
