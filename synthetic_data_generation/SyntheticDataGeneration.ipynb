{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Synthethic retail data generation\n",
    "\n",
    "> *This notebook works well with the `Data Science 2.0` and `Data Science` kernels in SageMaker Studio*\n",
    ">\n",
    "> ⚠️ *You'll need to use a **2 vCPU + 16 GiB (`ml.m5.xlarge`)** or similar instance. The kernel will run out of RAM on the default 2 vCPU + 4 GiB (`ml.t3.medium`)*\n",
    "\n",
    "This notebook generates a synthetic time series dataset to demonstrate demand forecasting in a retail context. The output (as detailed further in the [/README.md](../README.md)) includes:\n",
    "\n",
    "- Daily sales data\n",
    "- Static product metadata\n",
    "- Dynamic (monthly) product procurement/manufacture costs\n",
    "- Dynamic (daily) product sale prices\n",
    "- Other dynamic demand-driving features, such as public holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import dependencies and configure parameters\n",
    "\n",
    "We'll use some open-source libraries for time-series generation which aren't present in the standard SageMaker kernels, so you'll need to install those first if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install timeseries_generator workalendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the extra libraries installed, you're ready to import dependencies and set up configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import os\n",
    "import pickle  # nosec\n",
    "\n",
    "# External Dependencies:\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from timeseries_generator import (\n",
    "    Generator,\n",
    "    HolidayFactor,\n",
    "    LinearTrend,\n",
    "    RandomFeatureFactor,\n",
    "    SinusoidalFactor,\n",
    "    WeekdayFactor,\n",
    "    WhiteNoise,\n",
    ")\n",
    "from timeseries_generator.external_factors import CountryGdpFactor\n",
    "\n",
    "# Local Dependencies:\n",
    "import util\n",
    "\n",
    "# Set up data output folder (ignored by git):\n",
    "os.makedirs('../dataset', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Increase default plotting size for readability: (width, height)\n",
    "plt.rcParams['figure.figsize'] = (21, 6)\n",
    "\n",
    "# Seed random number generator for reproducibility:\n",
    "np.random.seed(seed=20060303)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "START_DATE = pd.Timestamp('2017-01-01')\n",
    "END_DATE = pd.Timestamp('2019-12-31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create factors\n",
    "\n",
    "The dataset is built up by defining, and then later combining, multiple contributing **dynamic \"factors\"** based on **static \"features\"**: For example the *feature* \"Country\" could drive a *time-varying factor* \"GDP\", and potentially others as well.\n",
    "\n",
    "Some of these features will affect demand directly, while others might drive item cost or pricing which drive demand indirectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEMAND_FEATURES = {}\n",
    "DEMAND_FACTORS = {}\n",
    "COST_FEATURES = {}\n",
    "COST_FACTORS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Countries (GDP, holidays, and business growth)\n",
    "\n",
    "Even single-country businesses are often affected by national holidays and macro-economic changes. In the example, we'll assume the business operates in a small group of countries with different conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "countries_config = {\n",
    "    'Brazil': {'growth': {'coef': 2, 'offset': -0.9}},\n",
    "    'Ireland': {'growth': {'coef': 1.0, 'offset': 2.0}},\n",
    "    'Japan': {'growth': {'coef': 0.4, 'offset': 1.6}},\n",
    "    'Netherlands': {'growth': {'coef': 1.2, 'offset': -0.2}},\n",
    "}\n",
    "\n",
    "DEMAND_FEATURES['country'] = [k for k in countries_config.keys()]\n",
    "COST_FEATURES['country'] = DEMAND_FEATURES['country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdp_factor = CountryGdpFactor(country_list=DEMAND_FEATURES['country'])\n",
    "DEMAND_FACTORS['gdp'] = gdp_factor\n",
    "COST_FACTORS['gdp'] = gdp_factor\n",
    "gdp_factor.plot(start_date=START_DATE, end_date=END_DATE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "holiday_factor = HolidayFactor(country_list=DEMAND_FEATURES['country'])\n",
    "holiday_factor.plot(start_date=START_DATE, end_date=END_DATE)\n",
    "DEMAND_FACTORS['holidays'] = holiday_factor\n",
    "COST_FACTORS['holidays'] = util.factors.transforms.scale_factor(\n",
    "    holiday_factor,\n",
    "    scale=0.5,\n",
    "    base=1.0,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ctry_growth_factor = LinearTrend(\n",
    "    feature='country',\n",
    "    feature_values={k: v['growth'] for k, v in countries_config.items()},\n",
    "    col_name='ctry_growth',\n",
    ")\n",
    "DEMAND_FACTORS['ctry_growth'] = ctry_growth_factor\n",
    "ctry_growth_factor.plot(start_date=START_DATE, end_date=END_DATE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-country store locations\n",
    "\n",
    "Most businesses will have more than one location in a particular country, so let's define a 'store' feature with a small set of possible values.\n",
    "\n",
    "In our case we'll re-use store IDs between countries: So an individual 'location' identifier would be a combination of store and country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stores_config = {'Store1', 'Store2', 'Store3'}\n",
    "\n",
    "DEMAND_FEATURES['store'] = sorted(s for s in stores_config)  # (Sort for reproducibility)\n",
    "\n",
    "store_random_factor = util.factors.RandomCompositeFeatureFactor(\n",
    "    feature_values={k: DEMAND_FEATURES[k] for k in ('country', 'store')},\n",
    "    col_name='store_random',\n",
    "    min_factor_value=0.1,\n",
    "    max_factor_value=3,\n",
    ")\n",
    "DEMAND_FACTORS['store_random'] = store_random_factor\n",
    "store_random_factor.plot(start_date=START_DATE, end_date=END_DATE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekday variation\n",
    "\n",
    "For simplicity, we'll assume here that the same weekday factors can be applied across all countries where the business operates. Of course there may be countries and industries where this isn't true!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weekday_factor = WeekdayFactor(\n",
    "    col_name='weekend_boost_factor',\n",
    "    factor_values={4: 1.15, 5: 1.3, 6: 1.3}  # Here we assign a factor of 1.15 to Friday, and 1.3 to Sat/Sun \n",
    ")\n",
    "DEMAND_FACTORS['weekday'] = weekday_factor\n",
    "weekday_factor.plot(start_date=START_DATE, end_date=END_DATE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item popularity and seasonality\n",
    "\n",
    "We'll define a set of actual product types here - each with:\n",
    "\n",
    "- Some manually-configured seasonal variation\n",
    "- A manually-configured base cost per item\n",
    "- A randomly-selected long-term trend\n",
    "- A randomly-selected short-term periodicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "products_config = {\n",
    "    'T-Shirt': util.config.ProductConfig(\n",
    "        seasonality=util.config.SeasonalityConfig(\n",
    "            amplitude=util.config.SeasonalityConfig.AMPLITUDE_DEFAULT / 0.5,\n",
    "        ),\n",
    "        base_cost=util.config.BaseCostConfig(initial=3.0, final_delta=0.4),\n",
    "    ),\n",
    "    'Hoodie': util.config.ProductConfig(\n",
    "        seasonality=util.config.SeasonalityConfig(\n",
    "            amplitude=util.config.SeasonalityConfig.AMPLITUDE_DEFAULT / 0.8,\n",
    "            phase=util.config.SeasonalityConfig.PHASE_ANNUAL_WINTER,\n",
    "        ),\n",
    "        base_cost=util.config.BaseCostConfig(initial=8.0, final_delta=0.8),\n",
    "    ),\n",
    "    'Sneakers': util.config.ProductConfig(\n",
    "        seasonality=util.config.SeasonalityConfig(\n",
    "            wavelength=util.config.SeasonalityConfig.WAVELENGTH_2SEASONS,\n",
    "        ),\n",
    "        base_cost=util.config.BaseCostConfig(initial=10.0, final_delta=1.8),\n",
    "    ),\n",
    "    'Jeans': util.config.ProductConfig(\n",
    "        seasonality=util.config.SeasonalityConfig(\n",
    "            amplitude=util.config.SeasonalityConfig.AMPLITUDE_DEFAULT * .6,\n",
    "            wavelength=util.config.SeasonalityConfig.WAVELENGTH_2SEASONS,\n",
    "        ),\n",
    "        base_cost=util.config.BaseCostConfig(initial=5.0, final_delta=0.7),\n",
    "    ),\n",
    "    'Scarf': util.config.ProductConfig(\n",
    "        seasonality=util.config.SeasonalityConfig(\n",
    "            amplitude=util.config.SeasonalityConfig.AMPLITUDE_DEFAULT * 1.8,\n",
    "            phase=util.config.SeasonalityConfig.PHASE_ANNUAL_WINTER + 15,\n",
    "        ),\n",
    "        base_cost=util.config.BaseCostConfig(initial=3.0, final_delta=-.2),\n",
    "    ),\n",
    "    'Gloves': util.config.ProductConfig(\n",
    "        seasonality=util.config.SeasonalityConfig(\n",
    "            amplitude=util.config.SeasonalityConfig.AMPLITUDE_DEFAULT * 2,\n",
    "            phase=util.config.SeasonalityConfig.PHASE_ANNUAL_WINTER + 30,\n",
    "        ),\n",
    "        base_cost=util.config.BaseCostConfig(initial=7.8, final_delta=1.2),\n",
    "    ),\n",
    "    'Sandals': util.config.ProductConfig(\n",
    "        base_cost=util.config.BaseCostConfig(initial=3.2, final_delta=0.3),\n",
    "    ),\n",
    "    'Socks': util.config.ProductConfig(\n",
    "        seasonality=util.config.SeasonalityConfig(\n",
    "            phase=util.config.SeasonalityConfig.PHASE_ANNUAL_WINTER,\n",
    "        ),\n",
    "        base_cost=util.config.BaseCostConfig(initial=2.4, final_delta=0.3),\n",
    "    ),\n",
    "}\n",
    "\n",
    "DEMAND_FEATURES['product'] = [p for p in products_config.keys()]\n",
    "COST_FEATURES['product'] = DEMAND_FEATURES['product']\n",
    "\n",
    "product_seasonality_factor = SinusoidalFactor(\n",
    "    feature='product',\n",
    "    col_name='product_seasonality',\n",
    "    feature_values={\n",
    "        name: cfg.seasonality.to_sinusoidal_factor_config()\n",
    "        for name, cfg in products_config.items()\n",
    "    },\n",
    ")\n",
    "DEMAND_FACTORS['product_seasonality'] = product_seasonality_factor\n",
    "COST_FACTORS['product_seasonality'] = util.factors.transforms.scale_factor(\n",
    "    product_seasonality_factor,\n",
    "    scale=0.5,\n",
    "    base=1.0,\n",
    ")\n",
    "product_seasonality_factor.plot(start_date=START_DATE, end_date=END_DATE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_longterm_sinusoid():\n",
    "    n_days_total = (END_DATE - START_DATE).days\n",
    "    wavelength = np.random.uniform(n_days_total, n_days_total * 5)\n",
    "    phase = np.random.uniform(-wavelength / 2, wavelength / 2)\n",
    "    amplitude = np.random.uniform(\n",
    "        util.config.SeasonalityConfig.AMPLITUDE_DEFAULT * 0.1,\n",
    "        util.config.SeasonalityConfig.AMPLITUDE_DEFAULT * 2.,\n",
    "    )\n",
    "    mean = np.random.uniform(\n",
    "        1. - amplitude / 2.,\n",
    "        1. + amplitude / 2.,\n",
    "    )\n",
    "    return {'amplitude': amplitude, 'mean': mean, 'phase': phase, 'wavelength': wavelength}\n",
    "\n",
    "\n",
    "product_longterm_factor = SinusoidalFactor(\n",
    "    feature='product',\n",
    "    col_name='product_longterm',\n",
    "    feature_values={\n",
    "        name: gen_longterm_sinusoid() for name in products_config.keys()\n",
    "    },\n",
    ")\n",
    "DEMAND_FACTORS['product_longterm'] = product_longterm_factor\n",
    "product_longterm_factor.plot(start_date=START_DATE, end_date=END_DATE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_shortterm_sinusoid():\n",
    "    wavelength = np.random.uniform(7, 90)\n",
    "    phase = np.random.uniform(-wavelength / 2, wavelength / 2)\n",
    "    amplitude = np.random.uniform(\n",
    "        util.config.SeasonalityConfig.AMPLITUDE_DEFAULT * 0.1,\n",
    "        util.config.SeasonalityConfig.AMPLITUDE_DEFAULT * 0.4,\n",
    "    )\n",
    "    mean = np.random.uniform(\n",
    "        1. - amplitude / 2.,\n",
    "        1. + amplitude / 2.,\n",
    "    )\n",
    "    return {'amplitude': amplitude, 'mean': mean, 'phase': phase, 'wavelength': wavelength}\n",
    "\n",
    "\n",
    "product_shortterm_factor = SinusoidalFactor(\n",
    "    feature='product',\n",
    "    col_name='product_shortterm',\n",
    "    feature_values={\n",
    "        name: gen_shortterm_sinusoid() for name in products_config.keys()\n",
    "    },\n",
    ")\n",
    "DEMAND_FACTORS['product_shortterm'] = product_shortterm_factor\n",
    "product_shortterm_factor.plot(start_date=START_DATE, end_date=END_DATE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_base_cost_factor = LinearTrend(\n",
    "    feature='product',\n",
    "    col_name='product_base_cost',\n",
    "    feature_values={\n",
    "        name: cfg.base_cost.to_linear_trend_config()\n",
    "        for name, cfg in products_config.items()\n",
    "    },\n",
    ")\n",
    "COST_FACTORS['product_base_cost'] = product_base_cost_factor\n",
    "product_base_cost_factor.plot(start_date=START_DATE, end_date=END_DATE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item size\n",
    "\n",
    "Since our example items above are in a fashion context, let's add an 'item_size' attribute driving more variation:\n",
    "\n",
    "> ⚠️ **Note:** `size` is a [reserved field name](https://docs.aws.amazon.com/forecast/latest/dg/reserved-field-names.html) not allowed in Amazon Forecast, hence we'll call this feature `item_size` to avoid potential problems later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sizes_config = {\n",
    "    'XL': {'offset': -.1},\n",
    "    'L': {'offset': 0.5},\n",
    "    'M': {'offset': 0.8},\n",
    "    'S': {'offset': 0.1},\n",
    "    'XS': {'offset': -0.3},\n",
    "}\n",
    "DEMAND_FEATURES['item_size'] = [k for k in sizes_config]\n",
    "\n",
    "\n",
    "def size_factor_values_from_config(size_config):\n",
    "    if not size_config:\n",
    "        size_config = {}\n",
    "    return {\n",
    "        'coef': size_config.get('coef', 0.0),\n",
    "        'offset': size_config.get('offset', 0.0),\n",
    "    }\n",
    "\n",
    "\n",
    "size_factor = LinearTrend(\n",
    "    feature='item_size',\n",
    "    feature_values={\n",
    "        k: size_factor_values_from_config(v) for k, v in sizes_config.items()\n",
    "    },\n",
    "    col_name='size_trend',\n",
    ")\n",
    "DEMAND_FACTORS['size_trend'] = size_factor\n",
    "size_factor.plot(start_date=START_DATE, end_date=END_DATE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item color\n",
    "\n",
    "...And similarly, a selection of colours in which each item might be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "colors_config = [\n",
    "    'red', 'green', 'blue', 'black', 'yellow', 'pink', 'brown', 'white', 'gray', 'navy',\n",
    "]\n",
    "\n",
    "DEMAND_FEATURES['color'] = colors_config\n",
    "\n",
    "color_random_factor = RandomFeatureFactor(\n",
    "    feature='color',\n",
    "    feature_values=DEMAND_FEATURES['color'],\n",
    "    min_factor_value=0.1,\n",
    "    max_factor_value=2.,\n",
    "    col_name='color_factor',\n",
    ")\n",
    "DEMAND_FACTORS['color_factor'] = color_random_factor\n",
    "color_random_factor.plot(start_date=START_DATE, end_date=END_DATE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pricing actions / promotions\n",
    "\n",
    "The sale price for each product will likely depend on its procurement/manufacture cost, plus some gross profit margin. Let's assume the margin is set by product type (independent of variations like size and colour), and doesn't directly drive demand because it's: A) static and B) based on the standard market price for the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_base_margin_factor = RandomFeatureFactor(\n",
    "    feature='product',\n",
    "    feature_values=DEMAND_FEATURES['product'],\n",
    "    min_factor_value=1.1,\n",
    "    max_factor_value=4.,\n",
    "    col_name='product_base_margin',\n",
    ")\n",
    "product_base_margin_factor.plot(start_date=START_DATE, end_date=END_DATE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Promotional discounts will be applied to products at various times through the year, and these certainly *should* drive changes in demand. In our model:\n",
    "\n",
    "- The discount percentage will be inversely related to the effect on demand/sales\n",
    "- The duration, amount, and distance between promotions per item will be randomly selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "demand_promos_factor = util.factors.RandomPromotionsFactor(\n",
    "    feature_values={'product': [name for name in products_config.keys()]},\n",
    "    random_seed=123,\n",
    ")\n",
    "DEMAND_FACTORS['promos'] = demand_promos_factor\n",
    "\n",
    "price_promos_factor = util.factors.transforms.scale_factor(\n",
    "    util.factors.transforms.invert_factor(demand_promos_factor),\n",
    "    scale=0.5,  # Price elasticity of demand (same across products in this simple case)\n",
    "    base=1.0,  # Scaling happens around 1.0\n",
    ")\n",
    "\n",
    "demand_promos_factor.plot(start_date=START_DATE, end_date=END_DATE)\n",
    "price_promos_factor.plot(start_date=START_DATE, end_date=END_DATE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random noise\n",
    "\n",
    "On top of these configured factors, some random noise will simulate natural, unpredictable variation in demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WHITENOISE_STDDEV = 0.1\n",
    "\n",
    "white_noise = WhiteNoise(stdev_factor=WHITENOISE_STDDEV)\n",
    "DEMAND_FACTORS['noise'] = white_noise\n",
    "white_noise.plot(start_date=START_DATE, end_date=END_DATE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate time-varying unit costs\n",
    "\n",
    "With the features and factors defined, we'll first generate time-series of unit costs per item.\n",
    "\n",
    "While the underlying factors affecting item cost may be daily in granularity, in practice the frequency of unit-cost changes a business observes may be limited: So we'll **aggregate unit costs by month** to illustrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "util.config.generation_size_diagnostic(COST_FEATURES, COST_FACTORS, START_DATE, END_DATE)\n",
    "\n",
    "g_cost = Generator(\n",
    "    factors=COST_FACTORS.values(),\n",
    "    features=COST_FEATURES,\n",
    "    date_range=pd.date_range(start=START_DATE, end=END_DATE),\n",
    "    base_value=1.0,\n",
    ")\n",
    "\n",
    "print('\\nGenerating item unit costs...')\n",
    "df_cost = g_cost.generate()\n",
    "\n",
    "print('Enforcing minimum unit cost value...')\n",
    "df_cost[df_cost['value'] < 0.1] = 0.1\n",
    "\n",
    "print('Grouping by month...')\n",
    "df_cost = df_cost.groupby(\n",
    "    [*COST_FEATURES.keys(), pd.PeriodIndex(df_cost['date'], freq='M')]\n",
    ").agg({'value': 'mean'})\n",
    "df_cost = df_cost.rename(columns={'value': 'unit_cost'})\n",
    "df_cost['unit_cost'] = df_cost['unit_cost'].round(decimals=2)\n",
    "\n",
    "display(df_cost)\n",
    "df_cost.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Saving generated cost data...')\n",
    "\n",
    "# Need to reset index for parquet or Pandas will not load the date index column correctly:\n",
    "df_cost.reset_index().to_parquet('../dataset/unit_costs.parquet')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assume unit cost is a factor driving sale price (below), but can't just directly reference the `COST_FACTORS` because we want to base price on the aggregated monthly figures (that our business would see from its suppliers), not the underlying variables (that our business would not directly see).\n",
    "\n",
    "Therefore we'll define this aggregated cost dataframe as a new kind of factor for the timeseries generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_cost_factor = util.factors.ExternalDateAggregatedFactor(\n",
    "    df_cost,\n",
    "    col_name=\"unit_cost\",\n",
    "    features=COST_FEATURES,\n",
    ")\n",
    "\n",
    "item_cost_factor.plot(start_date=START_DATE, end_date=END_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate item prices from cost and other factors\n",
    "\n",
    "We'll model the sale price of each item as being derived from the underlying unit cost, plus a margin, minus any promotional discounts.\n",
    "\n",
    "Note that if using product prices as an input to demand forecasting, you'll want to project out a scenario for future prices as well: Similarly to how we'll extend out holiday and weekend reference data beyond the base `END_DATE` in a later section. However, projecting out pricing scenarios is a bit more of an opinionated business decision than looking up public reference data: So we'll build this source data to stop at `END_DATE` and rely on forecasters to extend from there themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "PRICE_FEATURES = {**COST_FEATURES}\n",
    "PRICE_FACTORS = {\n",
    "    'unit_cost': item_cost_factor,\n",
    "    'base_margin': product_base_margin_factor,\n",
    "    'promos': price_promos_factor,\n",
    "}\n",
    "\n",
    "util.config.generation_size_diagnostic(PRICE_FEATURES, PRICE_FACTORS, START_DATE, END_DATE)\n",
    "\n",
    "g_price = Generator(\n",
    "    factors=PRICE_FACTORS.values(),\n",
    "    features=PRICE_FEATURES,\n",
    "    date_range=pd.date_range(start=START_DATE, end=END_DATE),\n",
    "    base_value=1.0,\n",
    ")\n",
    "\n",
    "print('\\nGenerating item price data...')\n",
    "df_price = g_price.generate()\n",
    "df_price = df_price.rename(columns={'value': 'unit_price', 'random_promos_factor': 'promo'})\n",
    "\n",
    "# Round output fields to avoid weirdly precise outputs:\n",
    "df_price['unit_price'] = df_price['unit_price'].round(decimals=2)\n",
    "df_price['promo'] = df_price['promo'].round(decimals=3)\n",
    "\n",
    "display(df_price)\n",
    "df_price.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since items are *usually* sold at their regular price, we can take a quick diagnostic of how many records are impacted by promotions and what the typical discount percentages are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_price[df_price['promo'] != 1.0]['promo'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When happy, this can be saved as our standard item price dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Saving generated price data...')\n",
    "price_export_cols = [\"date\", *PRICE_FEATURES.keys(), 'promo', 'unit_price']\n",
    "df_price[price_export_cols].to_parquet('../dataset/prices_promos.parquet', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't need this dataframe again in the notebook: Since price is fully under the business' control, we can just re-reference the underlying `PRICE_FACTORS` for sales generation. Hence we can delete the variable to free some memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del df_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate underlying demand\n",
    "\n",
    "With cost and pricing factors calculated, we're ready to simulate the *underlying demand for sales* for each item by day. (This is not quite the same as the final synthetic sales data, as we'll show in later sections)\n",
    "\n",
    "Setting the `base_value` controls the overall **sparsity** of the dataset, which is useful for simulating common retail patterns where e.g. there may be a long tail of SKUs with low sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_value = .8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the factors defined and a base value configured, we're ready to generate the time-series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "util.config.generation_size_diagnostic(DEMAND_FEATURES, DEMAND_FACTORS, START_DATE, END_DATE)\n",
    "\n",
    "g = Generator(\n",
    "    factors=DEMAND_FACTORS.values(),\n",
    "    features=DEMAND_FEATURES,\n",
    "    date_range=pd.date_range(start=START_DATE, end=END_DATE),\n",
    "    base_value=base_value,\n",
    ")\n",
    "\n",
    "print('\\nGenerating demand data...')\n",
    "df = g.generate()\n",
    "\n",
    "print('Enforcing non-negative integer output...')\n",
    "df['value'] = df['value'].astype(int)\n",
    "df[df['value'] < 0]['value'] = 0  # (This is not necessary unless any factors might have gone -ve)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save this raw file and the input configurations directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Saving configuration to file...')\n",
    "with open('../dataset/generator_config.pkl', 'wb') as fpkl:\n",
    "    pickle.dump(  # nosec\n",
    "        {\n",
    "            'cost_factors': COST_FACTORS,\n",
    "            'cost_features': COST_FEATURES,\n",
    "            'demand_factors': DEMAND_FACTORS,\n",
    "            'demand_features': DEMAND_FEATURES,\n",
    "            'price_factors': PRICE_FACTORS,\n",
    "            'price_features': PRICE_FEATURES,\n",
    "            'start': START_DATE,\n",
    "            'end': END_DATE,\n",
    "            'base_value': base_value,\n",
    "        },\n",
    "        fpkl,\n",
    "    )\n",
    "\n",
    "print('Saving generated data...')\n",
    "df.to_parquet('../dataset/demand_raw.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Load a previous generation)\n",
    "\n",
    "At this point we'll re-load the raw generation result from above:\n",
    "\n",
    "1. To check the integrity, and\n",
    "2. To provide a useful place to resume, in case later experimentation causes your kernel to run out of memory and restart!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Loading generated dataframe')\n",
    "df = pd.read_parquet('../dataset/demand_raw.parquet')\n",
    "\n",
    "with open('../dataset/generator_config.pkl', 'rb') as fpkl:\n",
    "    print('Loading configuration from file')\n",
    "    obj = pickle.load(fpkl)  # nosec\n",
    "    COST_FACTORS = obj['cost_factors']\n",
    "    COST_FEATURES = obj['cost_features']\n",
    "    DEMAND_FACTORS = obj['demand_factors']\n",
    "    DEMAND_FEATURES = obj['demand_features']\n",
    "    PRICE_FACTORS = obj['price_factors']\n",
    "    PRICE_FEATURES = obj['price_features']\n",
    "    START_DATE = obj['start']\n",
    "    END_DATE = obj['end']\n",
    "    base_value = obj['base_value']\n",
    "\n",
    "print('Ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Statistics of daily demand column:')\n",
    "print(df['value'].describe(), end='\\n\\n')\n",
    "\n",
    "n_zero_demands = (df['value'] == 0).sum()\n",
    "print(\n",
    "    'Zero sales on: {} records ({:.1%} of total)'.format(n_zero_demands, n_zero_demands / len(df))\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From demand to sales\n",
    "\n",
    "The raw generated demand table above needs some further transformation to create something like a sales database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic data shape\n",
    "\n",
    "First, from a functional perspective, the working columns where underlying factors were calculated of course shouldn't be exposed to forecasting. We also wouldn't see any record generated at all in sales systems on days where a particular item didn't sell *any* units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(\n",
    "    columns=['base_amount', 'total_factor'] + [f.col_name for f in DEMAND_FACTORS.values()],\n",
    "    inplace=True,\n",
    ")\n",
    "df.rename(columns={'value': 'demand'}, inplace=True)\n",
    "df = df.loc[df['demand'] != 0].copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping extreme low-volume items\n",
    "\n",
    "Second, from a business perspective, since the generation was random and sparsified there may be (location, product, variant) combinations with extremely low demand throughout the entire period.\n",
    "\n",
    "In practice, a most businesses simply wouldn't carry such extremely low-demand products over extended periods with regular re-ordering. A more likely scenario would be an initial trial order with no follow-up once demand was found to be too weak.\n",
    "\n",
    "We won't try to model new product introduction complexities in this sample, so will simply drop these *extreme* sparse cases.\n",
    "\n",
    "You could consider filtering either on the frequency of days having any sales (count), or the total number of sales (sum), or a combination of the two. We'll use the frequency of days, and drop any item/location combinations that were very rarely sold across the entire multi-year period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "demand_dims = [c for c in df.columns if c not in ('date', 'demand')]\n",
    "demand_record_counts = df.groupby(demand_dims)['demand'].count().sort_values()\n",
    "demand_record_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extreme_rare_sales = pd.Series(\n",
    "    True,\n",
    "    index=demand_record_counts[demand_record_counts < 10].index,\n",
    "    name='drop_rare',\n",
    ")\n",
    "print(\n",
    "    'Dropping %s extremely rarely-selling item/location combinations (out of %s total)'\n",
    "    % (len(extreme_rare_sales), len(demand_record_counts))\n",
    ")\n",
    "extreme_rare_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df, extreme_rare_sales.to_frame().reset_index(), how='left')\n",
    "df = df.loc[df['drop_rare'].isna()]\n",
    "df.drop(columns=['drop_rare'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Theory only) Modelling historical stock-outs\n",
    "\n",
    "For a really realistic treatment of inventory-related demand forecasting benefits, we should recognize that the business doesn't keep infinite stock on hand for every item, and therefore has probably *run out of stock* at various periods in the history.\n",
    "\n",
    "This would require simulating a baseline stock re-ordering process (such as a moving average demand forecast plus some margin and/or minimum \"safety stock\" level) through the demand history and demand for sales that couldn't be fulfilled.\n",
    "\n",
    "The resulting historical \"inventory available\" (0/1) status would be an important input feature for a demand forecasting model, as the model would need to learn the difference between periods of no sales due to low demand, versus periods of no sales where latent demand was high but stock was not available.\n",
    "\n",
    "We haven't yet implemented this stock-out modelling, but acknowledge it could be useful in future for improving realism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'demand': 'sales'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract and normalize datasets\n",
    "\n",
    "Having mapped from raw generated \"demand\" to a more realistic dataset of \"sales\", we're almost ready to save our result. All that remains is some field normalization:\n",
    "\n",
    "Usually, businesses will have some kind of item ID (for example the Stock Keeping Unit or SKU) which uniquely identifies a particular product variant. Likewise a store/location ID may or may not be unique across geographies.\n",
    "\n",
    "With Amazon Forecast in particular, there's a decision to be made between:\n",
    "\n",
    "- Splitting \"items\" as unique location-SKU combinations, to allow setting different metadata for one product across multiple locations, or\n",
    "- Treating SKU/product as the \"item\" identifier and location as a \"dimension\" of the forecast - so that one product shares metadata across all sale locations\n",
    "\n",
    "Either can be viable approaches, mainly depending whether there's a need to configure separate product metadata for different locations or not.\n",
    "\n",
    "In this sample we'll assume the only static product metadata used in forecasting is consistent across all locations - and use a single separate dimension to represent store location. In both cases the ID field will just be a concatenation of the underlying features - so you'll be able to recover the detail easily later if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "location_features = ['country', 'store']\n",
    "location_aggregations = {'location': {'features': location_features, 'sep': '_'}}\n",
    "sku_features = ['product', 'color', 'item_size']\n",
    "sku_aggregations = {'sku': {'features': sku_features, 'sep': '_'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create our \"item master\" product metadata table (from the sales data, to omit any products that were too sparse to feed through):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# De-duplicate all features (size faster than other group iterating methods):\n",
    "metadata_df = df.groupby([c for c in demand_dims if c not in location_features]).size()\n",
    "\n",
    "# Convert from a MultiIndex to df of values\n",
    "metadata_df = metadata_df.index.to_frame(index=False)\n",
    "\n",
    "# Aggregate the features together into a 'SKU' identifier:\n",
    "util.analytics.agg_df_features(metadata_df, sku_aggregations, drop_aggregated=False)\n",
    "\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A double-check just to make sure this identifier is unique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check SKU uniquely identifies records:\n",
    "n_skus = len(metadata_df['sku'].unique())\n",
    "if n_skus != len(metadata_df):\n",
    "    raise ValueError('sku does not uniquely identify records in the product metadata table!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...And if all's well, this static product metadata is ready to save and use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata_df.to_csv('../dataset/metadata.csv', index=False)\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, likewise apply aggregations to our sales data so that it's keyed only on the 'sku' product identifier and the combined location identifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "util.analytics.agg_df_features(df, sku_aggregations, drop_aggregated=True)\n",
    "util.analytics.agg_df_features(df, location_aggregations, drop_aggregated=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it may be worth checking the **overall sparsity** of the generated sales data, as this will affect the difficulty of the forecasting task. We can produce logarithmic plots to describe how many location-SKU combinations have at least N days with non-zero sales data through the history, and how many have at least X total units sold through the history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "axfreq = util.analytics.log_log_plot(\n",
    "    df.groupby(['sku', 'location'])['sales'].count().value_counts(),\n",
    "    title='Coverage over historical time period',\n",
    "    xlabel='Number of SKU-locations',\n",
    "    ylabel='Minimum days with sales',\n",
    "    # xnorm=True,  # Can optionally normalize from absolute count to 0-1 proportion of dataset\n",
    ")\n",
    "axfreq.get_figure().savefig('../dataset/item-coverage.png', bbox_inches='tight')\n",
    "\n",
    "plt.figure()\n",
    "axvol = util.analytics.log_log_plot(\n",
    "    df.groupby(['sku', 'location'])['sales'].sum().value_counts(),\n",
    "    title='Total sales volume',\n",
    "    xlabel='Number of SKU-locations',\n",
    "    ylabel='Minimum total units sold',\n",
    "    # xnorm=True,  # Can optionally normalize from absolute count to 0-1 proportion of dataset\n",
    ")\n",
    "axfreq.get_figure().savefig('../dataset/item-volume.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the data seems healthy, we at last we have our simulated sales system source data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_parquet('../dataset/sales.parquet', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract weekend and holiday dataset\n",
    "\n",
    "In addition to **static** item metadata, we'll later want to use some **dynamic** features to help build better forecasts.\n",
    "\n",
    "Public holidays were accounted for in the original timeseries generation, but the computed factor value is smoothed over multiple days by the library. It would be a bit of a cheat to use that representation directly in forecasting - so instead here we'll use the same underlying source to extract a list of holidays per country, and produce a more basic/custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from workalendar.america import Brazil\n",
    "from workalendar.asia import Japan\n",
    "from workalendar.europe import Ireland, Netherlands\n",
    "\n",
    "country_calendars = {\n",
    "    'Brazil': Brazil(),\n",
    "    'Ireland': Ireland(),\n",
    "    'Japan': Japan(),\n",
    "    'Netherlands': Netherlands(),\n",
    "}\n",
    "\n",
    "print('Countries in dataset:')\n",
    "print(DEMAND_FEATURES['country'])\n",
    "\n",
    "for country in DEMAND_FEATURES['country']:\n",
    "    if country not in country_calendars:\n",
    "        raise ValueError(f'Need to manually configure workalendar country \"{country}\" import')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, since our actual historical sales data goes to `end_date`, we'll want holiday data to extend beyond that and cover the forecast horizon - to make it a useful input for the forecasting model.\n",
    "\n",
    "The underlying public holiday list can only be fetched per year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_holiday_list(countries, start_date, end_date):\n",
    "    \"\"\"Doesn't actually filter by date - just year\"\"\"\n",
    "    dfs = []\n",
    "    for country in countries:\n",
    "        for year in range(start_date.year, end_date.year + 1):\n",
    "            cal = country_calendars[country].holidays(year)\n",
    "            dfs.append(\n",
    "                pd.DataFrame({\n",
    "                    'country': country,\n",
    "                    'date': pd.Series([hol[0] for hol in cal]).unique(),\n",
    "                })\n",
    "            )\n",
    "    return pd.concat(dfs, axis=0)\n",
    "\n",
    "\n",
    "holidays_df = build_holiday_list(\n",
    "    DEMAND_FEATURES['country'],\n",
    "    START_DATE,\n",
    "    END_DATE + pd.DateOffset(years=1),  # Include one year after the sales data ends\n",
    ")\n",
    "holidays_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the daily reference table, we'll first set up the days of the weekends (which are the same across countries in this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weekend_flag_df = WeekdayFactor(\n",
    "    col_name='weekend_flag',\n",
    "    factor_values={0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 1, 6: 1},\n",
    ").generate(\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE + pd.DateOffset(years=1, days=1),  # (Skips final Dec 31st without the offset)\n",
    ")\n",
    "weekend_flag_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...Then, add the holiday days by country as overrides in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "holiday_overrides = holidays_df.set_index(['country', 'date'])\n",
    "holiday_overrides['holiday_flag'] = 1\n",
    "\n",
    "# Replicate the weekend_flag_df across countries and set up its index:\n",
    "weekend_hol_df = weekend_flag_df.merge(\n",
    "    pd.Series(DEMAND_FEATURES['country'], name='country'),\n",
    "    how='cross',\n",
    ").set_index(['country', 'date'])\n",
    "\n",
    "# Join to holiday_overrides with the MultiIndex on [country, date]:\n",
    "weekend_hol_df = weekend_hol_df.join(holiday_overrides, how='left')\n",
    "weekend_hol_df['holiday_flag'] = weekend_hol_df['holiday_flag'].fillna(0).astype(int)\n",
    "\n",
    "# Create a composite field that's 2 on holidays, 1 on weekends, 0 otherwise:\n",
    "weekend_hol_df['weekend_hol_flag'] = np.where(\n",
    "    weekend_hol_df['holiday_flag'],\n",
    "    2,\n",
    "    weekend_hol_df['weekend_flag'],\n",
    ")\n",
    "weekend_hol_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combined feature is now ready to export:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weekend_hol_df.drop(columns=['weekend_flag', 'holiday_flag']).to_csv(\n",
    "    '../dataset/weekend_holiday_flag.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All done!\n",
    "\n",
    "The source data files are now saved in the [../dataset](../dataset) folder and ready to use.\n",
    "\n",
    "> ⚠️ **Note:** We've done our best to seed random number generators in this process for **reproducibility** assuming you restart the kernel before re-running the cells, and run the full notebook through start-to-end. However, it's better to be safe than sorry: Make sure to keep track of your dataset versions if experimenting with this code, and don't mix and match output files from different runs!"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1b6bb6d6730a9a6d7c45324eeb1e2833316373a3320b885ffc4c936ff9c917f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
